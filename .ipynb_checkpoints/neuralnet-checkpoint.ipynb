{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rede Neural\n",
    "Nesta tarefa você deve implementar uma rede neural com duas camadas totalmente conectadas. O treinamento e a validação deverão ser executados no CIFAR-10. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração inicial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rncvc.classifiers.neural_net import NeuralNet\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Permite a recarga automática de arquivos python importados\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" retorna erro relativo \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemente a rede neural na classe `NeuralNet` no arquivo `rncvc/classifiers/neural_net.py`. Os parâmetros (pesos) da rede são armazenados na variável (dicionário) `self.params`, onde as chaves são os nomes (string) das camadas e os valores são numpy arrays com os pesos. A seguir, inicializamos dados sintéticos e uma rede simplificada para guiar o procedimento de desenvolvimento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação de um modelo e um conjunto de dados para alguns testes.\n",
    "# Definimos um seed para que seja possivel a conferência dos resultados.\n",
    "\n",
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    np.random.seed(0)\n",
    "    return NeuralNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    np.random.seed(1)\n",
    "    X = 10 * np.random.randn(num_inputs, input_size)\n",
    "    y = np.array([0, 1, 2, 2, 1])\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward: calcule valores de predição para as classes\n",
    "A função `NeuralNet.loss` utiliza os dados e os parâmetros para calcular as predições para cada classe. Além disso, são calculados: o valor da função de custo e os gradientes dos parâmetros. \n",
    "\n",
    "Implemente a primeira parte, que faz a predição das classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Suas predicoes:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Predicoes corretas:\n",
      "[[-0.81233741 -1.27654624 -0.70335995]\n",
      " [-0.17129677 -1.18803311 -0.47310444]\n",
      " [-0.51590475 -1.01354314 -0.8504215 ]\n",
      " [-0.15419291 -0.48629638 -0.52901952]\n",
      " [-0.00618733 -0.12435261 -0.15226949]]\n",
      "\n",
      "Diferenca entre sua implementacao e os valores corretos:\n",
      "3.68027204961e-08\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X, reg=0.1)\n",
    "print 'Suas predicoes:'\n",
    "print scores\n",
    "print\n",
    "print 'Predicoes corretas:'\n",
    "correct_scores = np.asarray([\n",
    "  [-0.81233741, -1.27654624, -0.70335995],\n",
    "  [-0.17129677, -1.18803311, -0.47310444],\n",
    "  [-0.51590475, -1.01354314, -0.8504215 ],\n",
    "  [-0.15419291, -0.48629638, -0.52901952],\n",
    "  [-0.00618733, -0.12435261, -0.15226949]])\n",
    "print correct_scores\n",
    "print\n",
    "\n",
    "# A diferenca deve ser pequena. Normalmente < 1e-7\n",
    "print 'Diferenca entre sua implementacao e os valores corretos:'\n",
    "print np.sum(np.abs(scores - correct_scores))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward: calcule a função de custo\n",
    "\n",
    "Na mesma função, implemente a segunda parte, calculando os valores da função de custo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diferença da sua funcao de custo e do custo correto:\n",
      "1.79412040779e-13\n"
     ]
    }
   ],
   "source": [
    "loss, _ = net.loss(X, y, reg=0.1)\n",
    "correct_loss = 1.30378789133\n",
    "\n",
    "# deve ser pequena a diferenca, normalmente < 1e-12\n",
    "print 'Diferença da sua funcao de custo e do custo correto:'\n",
    "print np.sum(np.abs(loss - correct_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward\n",
    "\n",
    "Implemente o restante da função. Aqui os gradientes com respeito às variáveis `W1`, `b1`, `W2`, e `b2` devem ser calculados. \n",
    "Com uma implementação correta de todo o processo de backward você deve ser capaz de conferir utilizando o gradiente numérico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 erro relativo max: 3.669858e-09\n",
      "W2 erro relativo max: 3.440708e-09\n",
      "b2 erro relativo max: 3.865028e-11\n",
      "b1 erro relativo max: 2.738422e-09\n"
     ]
    }
   ],
   "source": [
    "from rncvc.gradient_check import eval_numerical_gradient\n",
    "\n",
    "# Use o gradiente numérico para verificar sua implementação da etapa backward.\n",
    "# Se sua implementação estiver correta, a diferença entre os gradientes será \n",
    "# inferior a 1e-8 para cada um das camadas de pesos: W1, W2, b1, b2.\n",
    "\n",
    "loss, grads = net.loss(X, y, reg=0.1)\n",
    "\n",
    "# as diferenças devem ser pequenas (<1e-8)\n",
    "for param_name in grads:\n",
    "    f = lambda W: net.loss(X, y, reg=0.1)[0]\n",
    "    param_grad_num = eval_numerical_gradient(f, net.params[param_name], verbose=False)\n",
    "    print '%s erro relativo max: %e' % (param_name, rel_error(param_grad_num, grads[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando a rede (modelo toy)\n",
    "\n",
    "Para treinar, utilize o SGD. Preencha os espaços em branco da função NeuralNet.train. Ainda, você precisa implementar a função NeuralNet.predict utilizada para verificar acurácia durante o treinamento. \n",
    "\n",
    "Com a implementação completa, executando o código abaixo, você deve ser capaz de obter o valor da função de custo próximo a 0.2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss de treinamento:  0.115509651984\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAHwCAYAAAAIDnN0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkiSpZAYySZKkkv0fDpBovqcpYaoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3a80c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "net = init_toy_model()\n",
    "stats = net.train(X, y, X, y,\n",
    "            learning_rate=1e-1, reg=0.1,\n",
    "            num_iters=100, verbose=False)\n",
    "\n",
    "print 'Loss de treinamento: ', stats['loss_history'][-1]\n",
    "\n",
    "# plotagem dos valores de custo durante o treinamento\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o CIFAR-10\n",
    "Com sua implementação completa (e correta), podemos carregar os dados do CIFAR-10 e usá-los para treinar um modelo de rede neuralem dados reais.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000L, 32L, 32L, 3L)\n",
      "(10000L, 32L, 32L, 3L)\n",
      "Shape dados treinamento:  (40000L, 3072L)\n",
      "Shape das classes (treinamento):  (40000L,)\n",
      "Shape dados validacao:  (10000L, 3072L)\n",
      "Shape das classes (validacao):  (10000L,)\n"
     ]
    }
   ],
   "source": [
    "from rncvc.data_utils import load_CIFAR10, save_model, load_model\n",
    "\n",
    "def get_CIFAR10_data():\n",
    "    \"\"\"\n",
    "    Carregando o CIFAR-10 e efetuando pré-processamento para preparar os dados\n",
    "    para entrada na Rede Neural.     \n",
    "    \"\"\"\n",
    "    # Carrega o CIFAR-10\n",
    "    cifar10_dir = 'rncvc/datasets/cifar-10-batches-py'    \n",
    "    X_train, y_train, X_valid, y_valid = load_CIFAR10(cifar10_dir)   \n",
    "\n",
    "    # Normalizacao dos dados: subtracao da imagem media\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_valid -= mean_image\n",
    "    \n",
    "    print X_train.shape\n",
    "    print X_valid.shape\n",
    "    \n",
    "    # Imagens para linhas \n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)    \n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "# Utiliza a funcao acima pra carregar os dados.\n",
    "X_train, y_train, X_valid, y_valid = get_CIFAR10_data()\n",
    "print 'Shape dados treinamento: ', X_train.shape\n",
    "print 'Shape das classes (treinamento): ', y_train.shape\n",
    "print 'Shape dados validacao: ', X_valid.shape\n",
    "print 'Shape das classes (validacao): ', y_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinando a Rede no CIFAR-10\n",
    "Para treinar a rede use SGD com momentum. Salve o melhor modelo com a função data_utils.save_model()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 10000: loss 2.303653\n",
      "iteration 100 / 10000: loss 2.302663\n",
      "iteration 200 / 10000: loss 2.293209\n",
      "iteration 300 / 10000: loss 2.233373\n",
      "iteration 400 / 10000: loss 2.077160\n",
      "iteration 500 / 10000: loss 2.144001\n",
      "iteration 600 / 10000: loss 2.144006\n",
      "iteration 700 / 10000: loss 2.007245\n",
      "iteration 800 / 10000: loss 2.027172\n",
      "iteration 900 / 10000: loss 1.988740\n",
      "iteration 1000 / 10000: loss 1.910138\n",
      "iteration 1100 / 10000: loss 1.990723\n",
      "iteration 1200 / 10000: loss 1.948012\n",
      "iteration 1300 / 10000: loss 1.886752\n",
      "iteration 1400 / 10000: loss 1.849721\n",
      "iteration 1500 / 10000: loss 1.796391\n",
      "iteration 1600 / 10000: loss 1.807220\n",
      "iteration 1700 / 10000: loss 1.952463\n",
      "iteration 1800 / 10000: loss 1.792296\n",
      "iteration 1900 / 10000: loss 1.818571\n",
      "iteration 2000 / 10000: loss 1.875826\n",
      "iteration 2100 / 10000: loss 1.735964\n",
      "iteration 2200 / 10000: loss 1.786504\n",
      "iteration 2300 / 10000: loss 1.789403\n",
      "iteration 2400 / 10000: loss 1.739135\n",
      "iteration 2500 / 10000: loss 1.694978\n",
      "iteration 2600 / 10000: loss 1.712579\n",
      "iteration 2700 / 10000: loss 1.708089\n",
      "iteration 2800 / 10000: loss 1.868751\n",
      "iteration 2900 / 10000: loss 1.870979\n",
      "iteration 3000 / 10000: loss 1.718116\n",
      "iteration 3100 / 10000: loss 1.708808\n",
      "iteration 3200 / 10000: loss 1.738150\n",
      "iteration 3300 / 10000: loss 1.746026\n",
      "iteration 3400 / 10000: loss 1.904360\n",
      "iteration 3500 / 10000: loss 1.764945\n",
      "iteration 3600 / 10000: loss 1.645155\n",
      "iteration 3700 / 10000: loss 1.566544\n",
      "iteration 3800 / 10000: loss 1.629391\n",
      "iteration 3900 / 10000: loss 1.707152\n",
      "iteration 4000 / 10000: loss 1.657938\n",
      "iteration 4100 / 10000: loss 1.665081\n",
      "iteration 4200 / 10000: loss 1.561073\n",
      "iteration 4300 / 10000: loss 1.651623\n",
      "iteration 4400 / 10000: loss 1.665191\n",
      "iteration 4500 / 10000: loss 1.596006\n",
      "iteration 4600 / 10000: loss 1.621504\n",
      "iteration 4700 / 10000: loss 1.648588\n",
      "iteration 4800 / 10000: loss 1.615194\n",
      "iteration 4900 / 10000: loss 1.753261\n",
      "iteration 5000 / 10000: loss 1.660624\n",
      "iteration 5100 / 10000: loss 1.547823\n",
      "iteration 5200 / 10000: loss 1.655417\n",
      "iteration 5300 / 10000: loss 1.571956\n",
      "iteration 5400 / 10000: loss 1.647313\n",
      "iteration 5500 / 10000: loss 1.566144\n",
      "iteration 5600 / 10000: loss 1.564724\n",
      "iteration 5700 / 10000: loss 1.672377\n",
      "iteration 5800 / 10000: loss 1.703301\n",
      "iteration 5900 / 10000: loss 1.628601\n",
      "iteration 6000 / 10000: loss 1.610402\n",
      "iteration 6100 / 10000: loss 1.619481\n",
      "iteration 6200 / 10000: loss 1.543039\n",
      "iteration 6300 / 10000: loss 1.563803\n",
      "iteration 6400 / 10000: loss 1.702535\n",
      "iteration 6500 / 10000: loss 1.697303\n",
      "iteration 6600 / 10000: loss 1.649804\n",
      "iteration 6700 / 10000: loss 1.592511\n",
      "iteration 6800 / 10000: loss 1.714713\n",
      "iteration 6900 / 10000: loss 1.662687\n",
      "iteration 7000 / 10000: loss 1.682227\n",
      "iteration 7100 / 10000: loss 1.548715\n",
      "iteration 7200 / 10000: loss 1.656173\n",
      "iteration 7300 / 10000: loss 1.576823\n",
      "iteration 7400 / 10000: loss 1.567649\n",
      "iteration 7500 / 10000: loss 1.524444\n",
      "iteration 7600 / 10000: loss 1.652988\n",
      "iteration 7700 / 10000: loss 1.676166\n",
      "iteration 7800 / 10000: loss 1.531695\n",
      "iteration 7900 / 10000: loss 1.608446\n",
      "iteration 8000 / 10000: loss 1.652145\n",
      "iteration 8100 / 10000: loss 1.568016\n",
      "iteration 8200 / 10000: loss 1.547221\n",
      "iteration 8300 / 10000: loss 1.583232\n",
      "iteration 8400 / 10000: loss 1.706468\n",
      "iteration 8500 / 10000: loss 1.560599\n",
      "iteration 8600 / 10000: loss 1.678193\n",
      "iteration 8700 / 10000: loss 1.604705\n",
      "iteration 8800 / 10000: loss 1.597598\n",
      "iteration 8900 / 10000: loss 1.523938\n",
      "iteration 9000 / 10000: loss 1.583803\n",
      "iteration 9100 / 10000: loss 1.578504\n",
      "iteration 9200 / 10000: loss 1.570237\n",
      "iteration 9300 / 10000: loss 1.584840\n",
      "iteration 9400 / 10000: loss 1.510835\n",
      "iteration 9500 / 10000: loss 1.672636\n",
      "iteration 9600 / 10000: loss 1.661486\n",
      "iteration 9700 / 10000: loss 1.633222\n",
      "iteration 9800 / 10000: loss 1.632042\n",
      "iteration 9900 / 10000: loss 1.536996\n",
      "Acuracia de validacao:  0.4324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 144\n",
    "num_classes = 10\n",
    "net = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "\n",
    "# Treina a rede\n",
    "stats = net.train(X_train, y_train, X_valid, y_valid,\n",
    "            num_iters=10000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.5, verbose=True)\n",
    "\n",
    "# Efetua predicao no conjunto de validacao\n",
    "val_acc = (net.predict(X_valid) == y_valid).mean()\n",
    "print 'Acuracia de validacao: ', val_acc\n",
    "\n",
    "# Salva o modelo da rede treinada\n",
    "model_path = 'model.pickle'\n",
    "save_model(model_path, net.params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acompanhando o treinamento\n",
    "\n",
    "Com os hiperparâmetros definidos anteriormente você provavelmente não obterá bons resultados: a acurácia não deve passar de 30%. \n",
    "\n",
    "Uma estratégia para entender o que não está bom durante o treinamento é plotar os valores da função de custo e acurácia durante o treinamento. \n",
    "\n",
    "Outra estratégia é visualizar os pesos aprendidos na primeira camada durante o processo de otimização. Normalmente, redes treinadas em dados visuais apresentam padrões estruturais visíveis na primeira camada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plota a função de custo e acurácia\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(stats['loss_history'])\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(stats['train_acc_history'], label='train')\n",
    "plt.plot(stats['val_acc_history'], label='val')\n",
    "plt.title('Classification accuracy history')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Clasification accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rncvc.vis_utils import visualize_grid\n",
    "\n",
    "# Visualiza os pesos da rede\n",
    "\n",
    "def show_net_weights(net):\n",
    "    W1 = net.params['W1']\n",
    "    W1 = W1.reshape(32, 32, 3, -1).transpose(3, 0, 1, 2)\n",
    "    plt.imshow(visualize_grid(W1, padding=3).astype('uint8'))\n",
    "    plt.gca().axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_net_weights(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio RNCVC 2017\n",
    "\n",
    "**Dicas**\n",
    "* Tune os hiperparâmetros (taxa de aprendizado, número de neurônios, etc). \n",
    "* Regularize o modelo: teste o uso de Regularização L2 e Dropout. \n",
    "* Use validação cruzada para achar os melhores hiperparâmetros.\n",
    "* Utilize um esquema mais sofisticado de atualização dos pesos (ex: SGD + Momentum, Nesterov, Adadelta, Adam, RMSProp).\n",
    "* Teste funções de ativação (ex: ELU, Leaky ReLU) e Batch Normalization. \n",
    "* Salve o melhor modelo obtido usando data_utils.save_model() e data_utils.load_model() para abrir.\n",
    "* O campeão do ano passado conseguiu 62% de acurácia! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o desafio: é <b>fundamental</b> que o trecho abaixo seja executado sem problemas. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Assuma que o modelo são os pesos serializados em disco usando data_utils.save_model()\n",
    "input_size = 32 * 32 * 3\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "net = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "# Treina a rede\n",
    "stats = net.train(X_train, y_train, X_valid, y_valid,\n",
    "            num_iters=1000, batch_size=200,\n",
    "            learning_rate=1e-4, learning_rate_decay=0.95,\n",
    "            reg=0.5, verbose=True)\n",
    "\n",
    "best_model = 'path/to/best/model.pickle'\n",
    "model = load_model(best_model)\n",
    "net.params = model\n",
    "\n",
    "# Retorna um vetor de predição (N x 1), onde N é o número de instâncias\n",
    "# As classes retornadas aqui devem ser inteiros: [0, 1, 2, ..., C]\n",
    "# Assuma que X_teste e uma matriz de instancias (N_test, D) \n",
    "# Voce nao tera o X_teste oficial, use outro conjunto para validar.\n",
    "predicted_classes = net.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporte abaixo os valores de acurácia no treino e validação do modelo enviado para o desafio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Treino: 00.00%\n",
    "# Validacao: 00.00%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores1 = [[-2.85, 0.86, 0.28]]\n",
    "\n",
    "N1 = 1\n",
    "Y = [1,0]\n",
    "reg = 0.5\n",
    "\n",
    "exp_scores2 = np.exp(scores1)\n",
    "norm =(exp_scores2 / np.sum(exp_scores2, axis=1, keepdims=True)) # [N x K]\n",
    "\n",
    "# compute softmax loss\n",
    "loss = -np.sum(np.log(norm[np.arange(N1), Y]))\n",
    "loss /= N\n",
    "#loss += 0.5 * reg * (np.sum(W1**2) + np.sum(W2**2))\n",
    "\n",
    "print norm, loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
