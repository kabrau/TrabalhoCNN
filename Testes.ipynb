{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from rncvc.classifiers.neural_net import NeuralNet\n",
    "from rncvc.data_utils import load_CIFAR10, save_model, load_model\n",
    "\n",
    "# Permite a recarga automÃ¡tica de arquivos python importados\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" retorna erro relativo \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def get_CIFAR10_data():\n",
    "    \"\"\"\n",
    "    Carregando o CIFAR-10 e efetuando pre-processamento para preparar os dados\n",
    "    para entrada na Rede Neural.     \n",
    "    \"\"\"\n",
    "    # Carrega o CIFAR-10\n",
    "    cifar10_dir = 'rncvc/datasets/cifar-10-batches-py'    \n",
    "    X_train, y_train, X_valid, y_valid = load_CIFAR10(cifar10_dir)   \n",
    "\n",
    "    # Normalizacao dos dados: subtracao da imagem media\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_valid -= mean_image\n",
    "    \n",
    "    # Imagens para linhas \n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_valid = X_valid.reshape(X_valid.shape[0], -1)    \n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "# Utiliza a funcao acima pra carregar os dados.\n",
    "X_train, y_train, X_valid, y_valid = get_CIFAR10_data()\n",
    "input_size = 32 * 32 * 3\n",
    "num_classes = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 -> loss 1.794972 acc 0.395000 val_loss 1.876321 val_acc 0.329000  LossVal LossTrain AccValid\n",
      "Epoch 1 -> loss 1.886575 acc 0.405000 val_loss 1.778529 val_acc 0.379500  LossVal AccValid\n",
      "Epoch 2 -> loss 1.786336 acc 0.490000 val_loss 1.731964 val_acc 0.398600  LossVal LossTrain AccValid\n",
      "Epoch 3 -> loss 1.814296 acc 0.460000 val_loss 1.710812 val_acc 0.413800  LossVal AccValid\n",
      "Epoch 4 -> loss 1.806926 acc 0.445000 val_loss 1.694658 val_acc 0.426400  LossVal AccValid\n",
      "Epoch 5 -> loss 1.702100 acc 0.525000 val_loss 1.680898 val_acc 0.427300  LossVal LossTrain AccValid\n",
      "Epoch 6 -> loss 1.738211 acc 0.490000 val_loss 1.671673 val_acc 0.436100  LossVal AccValid\n",
      "Epoch 7 -> loss 1.666015 acc 0.495000 val_loss 1.666633 val_acc 0.436700  LossVal LossTrain AccValid\n",
      "Epoch 8 -> loss 1.769148 acc 0.455000 val_loss 1.658848 val_acc 0.431800  LossVal\n",
      "Epoch 9 -> loss 1.711900 acc 0.515000 val_loss 1.660696 val_acc 0.439600  AccValid\n",
      "Epoch 10 -> loss 1.563257 acc 0.585000 val_loss 1.657391 val_acc 0.434500  LossVal LossTrain\n",
      "Epoch 11 -> loss 1.819648 acc 0.530000 val_loss 1.658447 val_acc 0.441000  AccValid\n",
      "Epoch 12 -> loss 1.715112 acc 0.530000 val_loss 1.649819 val_acc 0.446000  LossVal AccValid\n",
      "Epoch 13 -> loss 1.809136 acc 0.495000 val_loss 1.653067 val_acc 0.443900 \n",
      "Epoch 14 -> loss 1.695580 acc 0.555000 val_loss 1.643896 val_acc 0.443900  LossVal\n",
      "Epoch 15 -> loss 1.713363 acc 0.515000 val_loss 1.641632 val_acc 0.448200  LossVal AccValid\n",
      "Epoch 16 -> loss 1.696740 acc 0.555000 val_loss 1.636600 val_acc 0.451600  LossVal AccValid\n",
      "Epoch 17 -> loss 1.704511 acc 0.520000 val_loss 1.639502 val_acc 0.447400 \n",
      "Epoch 18 -> loss 1.766497 acc 0.485000 val_loss 1.636869 val_acc 0.452100  AccValid\n",
      "Epoch 19 -> loss 1.675021 acc 0.555000 val_loss 1.638028 val_acc 0.453000  AccValid\n",
      "Epoch 20 -> loss 1.775649 acc 0.450000 val_loss 1.631105 val_acc 0.457700  LossVal AccValid\n",
      "Epoch 21 -> loss 1.650191 acc 0.520000 val_loss 1.638110 val_acc 0.448600 \n",
      "Epoch 22 -> loss 1.762066 acc 0.505000 val_loss 1.635839 val_acc 0.452400 \n",
      "Epoch 23 -> loss 1.754052 acc 0.470000 val_loss 1.629650 val_acc 0.452000  LossVal\n",
      "Epoch 24 -> loss 1.686220 acc 0.565000 val_loss 1.626175 val_acc 0.457600  LossVal\n",
      "Epoch 25 -> loss 1.652889 acc 0.535000 val_loss 1.622734 val_acc 0.452000  LossVal\n",
      "Epoch 26 -> loss 1.747267 acc 0.510000 val_loss 1.625141 val_acc 0.459200  AccValid\n",
      "Epoch 27 -> loss 1.816071 acc 0.475000 val_loss 1.629424 val_acc 0.457100 \n",
      "Epoch 28 -> loss 1.694566 acc 0.530000 val_loss 1.625625 val_acc 0.458500 \n",
      "Epoch 29 -> loss 1.745902 acc 0.500000 val_loss 1.626100 val_acc 0.464200  AccValid\n",
      "Epoch 30 -> loss 1.804543 acc 0.465000 val_loss 1.629745 val_acc 0.452700 \n",
      "Epoch 31 -> loss 1.611868 acc 0.540000 val_loss 1.623598 val_acc 0.456900 \n",
      "Epoch 32 -> loss 1.799477 acc 0.460000 val_loss 1.622316 val_acc 0.462600  LossVal\n",
      "Epoch 33 -> loss 1.787967 acc 0.475000 val_loss 1.628945 val_acc 0.457400 \n",
      "Epoch 34 -> loss 1.853615 acc 0.450000 val_loss 1.622665 val_acc 0.457500 \n",
      "Epoch 35 -> loss 1.806593 acc 0.485000 val_loss 1.624230 val_acc 0.460000 \n",
      "Epoch 36 -> loss 1.587586 acc 0.600000 val_loss 1.616976 val_acc 0.463100  LossVal\n",
      "Epoch 37 -> loss 1.766393 acc 0.520000 val_loss 1.616345 val_acc 0.462500  LossVal\n",
      "Epoch 38 -> loss 1.757308 acc 0.490000 val_loss 1.616946 val_acc 0.463400 \n",
      "Epoch 39 -> loss 1.786045 acc 0.500000 val_loss 1.617780 val_acc 0.461000 \n",
      "Epoch 40 -> loss 1.748688 acc 0.450000 val_loss 1.614632 val_acc 0.464900  LossVal AccValid\n",
      "Epoch 41 -> loss 1.592997 acc 0.565000 val_loss 1.615893 val_acc 0.465500  AccValid\n",
      "Epoch 42 -> loss 1.666930 acc 0.510000 val_loss 1.615377 val_acc 0.465300 \n",
      "Epoch 43 -> loss 1.713056 acc 0.520000 val_loss 1.616538 val_acc 0.464300 \n",
      "Epoch 44 -> loss 1.706053 acc 0.470000 val_loss 1.614517 val_acc 0.465900  LossVal AccValid\n",
      "Epoch 45 -> loss 1.720628 acc 0.475000 val_loss 1.612489 val_acc 0.467600  LossVal AccValid\n",
      "Epoch 46 -> loss 1.711973 acc 0.515000 val_loss 1.612214 val_acc 0.466100  LossVal\n",
      "Epoch 47 -> loss 1.529951 acc 0.595000 val_loss 1.610301 val_acc 0.464900  LossVal LossTrain\n",
      "Epoch 48 -> loss 1.671578 acc 0.550000 val_loss 1.614360 val_acc 0.465500 \n",
      "Epoch 49 -> loss 1.594619 acc 0.540000 val_loss 1.609683 val_acc 0.471600  LossVal AccValid\n",
      "Epoch 50 -> loss 1.683424 acc 0.495000 val_loss 1.616030 val_acc 0.464300 \n",
      "Epoch 51 -> loss 1.656555 acc 0.565000 val_loss 1.609352 val_acc 0.468800  LossVal\n",
      "Epoch 52 -> loss 1.770862 acc 0.490000 val_loss 1.611756 val_acc 0.467500 \n",
      "Epoch 53 -> loss 1.667371 acc 0.530000 val_loss 1.613849 val_acc 0.466600 \n",
      "Epoch 54 -> loss 1.754434 acc 0.505000 val_loss 1.608345 val_acc 0.469000  LossVal\n",
      "Epoch 55 -> loss 1.682404 acc 0.540000 val_loss 1.612985 val_acc 0.465200 \n",
      "Epoch 56 -> loss 1.647309 acc 0.510000 val_loss 1.610660 val_acc 0.464300 \n",
      "Epoch 57 -> loss 1.771607 acc 0.485000 val_loss 1.608443 val_acc 0.471500 \n",
      "Epoch 58 -> loss 1.716889 acc 0.545000 val_loss 1.611107 val_acc 0.467100 \n",
      "Epoch 59 -> loss 1.539478 acc 0.575000 val_loss 1.610801 val_acc 0.464900 \n",
      "Epoch 60 -> loss 1.841847 acc 0.465000 val_loss 1.605858 val_acc 0.474400  LossVal AccValid\n",
      "Epoch 61 -> loss 1.770178 acc 0.480000 val_loss 1.606296 val_acc 0.474300 \n",
      "Epoch 62 -> loss 1.625570 acc 0.570000 val_loss 1.608877 val_acc 0.468600 \n",
      "Epoch 63 -> loss 1.773972 acc 0.435000 val_loss 1.603437 val_acc 0.473800  LossVal\n",
      "Epoch 64 -> loss 1.671132 acc 0.540000 val_loss 1.608397 val_acc 0.475200  AccValid\n",
      "Epoch 65 -> loss 1.740256 acc 0.510000 val_loss 1.603240 val_acc 0.475500  LossVal AccValid\n",
      "Epoch 66 -> loss 1.629554 acc 0.540000 val_loss 1.605354 val_acc 0.471700 \n",
      "Epoch 67 -> loss 1.669883 acc 0.510000 val_loss 1.606320 val_acc 0.472400 \n",
      "Epoch 68 -> loss 1.682223 acc 0.525000 val_loss 1.606991 val_acc 0.469900 \n",
      "Epoch 69 -> loss 1.589160 acc 0.565000 val_loss 1.605847 val_acc 0.472600 \n",
      "Epoch 70 -> loss 1.698311 acc 0.510000 val_loss 1.607748 val_acc 0.472500 \n",
      "Epoch 71 -> loss 1.673969 acc 0.475000 val_loss 1.604993 val_acc 0.471600 \n",
      "Epoch 72 -> loss 1.716876 acc 0.535000 val_loss 1.607444 val_acc 0.472700 \n",
      "Epoch 73 -> loss 1.573193 acc 0.540000 val_loss 1.604808 val_acc 0.473200 \n",
      "Epoch 74 -> loss 1.685624 acc 0.480000 val_loss 1.604817 val_acc 0.473300 \n",
      "Epoch 75 -> loss 1.603503 acc 0.570000 val_loss 1.606194 val_acc 0.473300 \n"
     ]
    }
   ],
   "source": [
    "earlyStopping = 10\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 200\n",
    "hidden_size = 50\n",
    "learning_rate=1e-3\n",
    "learning_rate_decay=0.95\n",
    "reg = 0.5\n",
    "\n",
    "net = NeuralNet(input_size, hidden_size, num_classes)\n",
    "\n",
    "stats = net.trainMarcelo(X_train, y_train, X_valid, y_valid,\n",
    "                                    epochs=epochs,\n",
    "                                    batch_size=batch_size,\n",
    "                                    learning_rate=learning_rate, \n",
    "                                    learning_rate_decay=learning_rate_decay,\n",
    "                                    reg=reg, \n",
    "                                    verbose=True, \n",
    "                                    earlyStopping=earlyStopping,\n",
    "                                    dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.48204337,  0.38919656,  0.08748992,  0.93131978,  0.99082602,\n",
       "        0.89071139,  0.70776534,  0.56167678,  0.91818596,  0.81350967,\n",
       "        0.26919781,  0.31064515,  0.39079426,  0.22860788,  0.37554813,\n",
       "        0.11018686,  0.54475481,  0.32090597,  0.92987917,  0.83920129,\n",
       "        0.32122042,  0.08155545,  0.97552366,  0.01697981,  0.98634913,\n",
       "        0.75178928,  0.05947261,  0.66630409,  0.12789934,  0.88351512,\n",
       "        0.70754686,  0.15454449,  0.00826729,  0.4948463 ,  0.25870363,\n",
       "        0.01933402,  0.93908066,  0.75858291,  0.15215997,  0.51543434,\n",
       "        0.7853732 ,  0.58142474,  0.89383953,  0.35361429,  0.32538133,\n",
       "        0.35811837,  0.24287839,  0.20406203,  0.94313486,  0.65719784,\n",
       "        0.12965335,  0.16063855,  0.63520728,  0.42071151,  0.66375747,\n",
       "        0.21772558,  0.72263567,  0.8445806 ,  0.33745164,  0.28094646,\n",
       "        0.9338171 ,  0.18722165,  0.49826041,  0.68831432,  0.49995148,\n",
       "        0.87428117,  0.00840855,  0.61841425,  0.1113491 ,  0.18037123,\n",
       "        0.35019125,  0.53383469,  0.94543682,  0.98036399,  0.56954456,\n",
       "        0.98256088,  0.14863245,  0.06708394,  0.88926465,  0.71271331,\n",
       "        0.68245976,  0.20593295,  0.82933596,  0.22087456,  0.24253435,\n",
       "        0.99387345,  0.09145757,  0.00856522,  0.09828447,  0.03554751,\n",
       "        0.17922411,  0.77210185,  0.29642026,  0.89455459,  0.60678612,\n",
       "        0.12562661,  0.58711079,  0.38554991,  0.47353025,  0.99364749,\n",
       "        0.29853466,  0.70700804,  0.0399896 ,  0.3173882 ,  0.31173396,\n",
       "        0.95955098,  0.1605012 ,  0.87140759,  0.16907808,  0.76400298,\n",
       "        0.39179612,  0.70818243,  0.64237174,  0.49261678,  0.4860292 ,\n",
       "        0.45586179,  0.49862973,  0.04722892,  0.11552252,  0.53953594,\n",
       "        0.10313071,  0.26319026,  0.38191765,  0.60832674,  0.53288348,\n",
       "        0.54080255,  0.72487237,  0.61617428,  0.54392345,  0.8594305 ,\n",
       "        0.82720494,  0.35581532,  0.04034812,  0.39716522,  0.91927017,\n",
       "        0.24624265,  0.12160557,  0.05577221,  0.73378177,  0.21498889,\n",
       "        0.0613753 ,  0.87674326,  0.23398411,  0.67626534,  0.6498107 ,\n",
       "        0.20458826,  0.1263459 ,  0.79267171,  0.9420295 ,  0.02550448,\n",
       "        0.19791326,  0.69942706,  0.53105747,  0.38695017,  0.99548317,\n",
       "        0.85325901,  0.40258307,  0.34966967,  0.18690019,  0.00170484,\n",
       "        0.99606101,  0.84002745,  0.20687652,  0.53958422,  0.38268638,\n",
       "        0.04111236,  0.55971114,  0.12385587,  0.22250529,  0.31446911,\n",
       "        0.40609585,  0.81590855,  0.95887399,  0.53102327,  0.41690099,\n",
       "        0.32901656,  0.23433403,  0.03506236,  0.23874904,  0.06173145,\n",
       "        0.43155551,  0.48411724,  0.42623309,  0.05043964,  0.81033354,\n",
       "        0.10530992,  0.14757064,  0.04780445,  0.05783412,  0.49720008,\n",
       "        0.85809043,  0.10236405,  0.24394653,  0.0883897 ,  0.48385047,\n",
       "        0.78370004,  0.9077089 ,  0.21455844,  0.7455807 ,  0.14566283,\n",
       "        0.54741148,  0.02649913,  0.71717975,  0.96454808,  0.01550691,\n",
       "        0.65559306,  0.99179172,  0.99543483,  0.48582741,  0.62577694,\n",
       "        0.7361682 ,  0.83114373,  0.38573522,  0.11966621,  0.06238442,\n",
       "        0.89237322,  0.52981695,  0.24153307,  0.39064357,  0.59062537,\n",
       "        0.74471986,  0.55982752,  0.27280772,  0.54457103,  0.35324284,\n",
       "        0.59975526,  0.8608648 ,  0.11292054,  0.19048435,  0.0085379 ,\n",
       "        0.91391892,  0.68460506,  0.86319184,  0.63617174,  0.79971902,\n",
       "        0.17135372,  0.80891311,  0.99676253,  0.07073842,  0.91786387,\n",
       "        0.29652116,  0.08623257,  0.59803553,  0.93055652,  0.28647798,\n",
       "        0.64158655,  0.66676893,  0.3318896 ,  0.26017907,  0.90307813,\n",
       "        0.61094671,  0.32824859,  0.33986962,  0.10382194,  0.52190173,\n",
       "        0.75265947,  0.56599643,  0.11824598,  0.27276435,  0.73297208,\n",
       "        0.26756884,  0.47978023,  0.79901934,  0.4934859 ,  0.54964709,\n",
       "        0.14886398,  0.36777004,  0.38577112,  0.31829964,  0.62802478,\n",
       "        0.79078583,  0.29335753,  0.48551814,  0.8045047 ,  0.57054918,\n",
       "        0.81868643,  0.93447397,  0.86357174,  0.34638247,  0.07038019,\n",
       "        0.98476187,  0.69385678,  0.08678478,  0.62098383,  0.6206048 ,\n",
       "        0.46139651,  0.96696939,  0.56665849,  0.426909  ,  0.64031084,\n",
       "        0.03641384,  0.955954  ,  0.37209079,  0.00655251,  0.91948536,\n",
       "        0.41329968,  0.75980805,  0.30035105,  0.67228805,  0.51309378,\n",
       "        0.21864059,  0.1465905 ,  0.58280127,  0.52050502,  0.75065921,\n",
       "        0.45680376,  0.07371567,  0.51527354,  0.96947713,  0.04946674,\n",
       "        0.36529405,  0.20147922,  0.33859777,  0.98860443,  0.41557368,\n",
       "        0.59947865,  0.77927315,  0.13428946,  0.74895046,  0.04522938,\n",
       "        0.42502222,  0.77068866,  0.6887324 ,  0.23210048,  0.67751439,\n",
       "        0.84009343,  0.78259266,  0.31704833,  0.29927865,  0.75249519,\n",
       "        0.07827189,  0.57787981,  0.77256356,  0.16092896,  0.00545397,\n",
       "        0.79363323,  0.62410475,  0.51295811,  0.11968101,  0.6608011 ,\n",
       "        0.40908262,  0.10616489,  0.28343792,  0.64745282,  0.38408789,\n",
       "        0.16988861,  0.28270201,  0.40638569,  0.87055018,  0.32819412,\n",
       "        0.30885459,  0.20036815,  0.2788336 ,  0.75451762,  0.74998502,\n",
       "        0.97504564,  0.86339134,  0.82907677,  0.2817552 ,  0.87534652,\n",
       "        0.25690489,  0.57399264,  0.97186579,  0.51267107,  0.12142599,\n",
       "        0.98461659,  0.30809968,  0.37005838,  0.29219241,  0.99033028,\n",
       "        0.84723997,  0.36888356,  0.15806352,  0.63276185,  0.20622472,\n",
       "        0.89794518,  0.26747443,  0.90822427,  0.88448979,  0.65075255,\n",
       "        0.16362818,  0.92366944,  0.98861378,  0.03107022,  0.13981728,\n",
       "        0.49992842,  0.3639112 ,  0.12926987,  0.60802286,  0.91683377,\n",
       "        0.32669478,  0.75145698,  0.79454578,  0.60697912,  0.60441017,\n",
       "        0.19418937,  0.49847154,  0.07474423,  0.11944612,  0.30345254,\n",
       "        0.12389738,  0.13620861,  0.15617515,  0.56703765,  0.61968953,\n",
       "        0.37272646,  0.3938099 ,  0.88730201,  0.05959278,  0.17842464,\n",
       "        0.56230108,  0.2826546 ,  0.02360126,  0.4654244 ,  0.72619042,\n",
       "        0.83279536,  0.37704231,  0.63697937,  0.63029115,  0.67892923,\n",
       "        0.02273862,  0.33216487,  0.17774471,  0.01140322,  0.56139904,\n",
       "        0.45764224,  0.92612499,  0.23577951,  0.47844159,  0.05091292,\n",
       "        0.27878516,  0.55592481,  0.68921464,  0.36556614,  0.48925011,\n",
       "        0.78751916,  0.54893929,  0.00237195,  0.91615499,  0.15065914,\n",
       "        0.81103132,  0.78895933,  0.29470523,  0.14733748,  0.96667364,\n",
       "        0.94637386,  0.51179052,  0.84415659,  0.43255065,  0.89708461,\n",
       "        0.55690433,  0.3567322 ,  0.89955709,  0.98706586,  0.94645703,\n",
       "        0.42393269,  0.41918359,  0.80820786,  0.24931151,  0.47367814,\n",
       "        0.10858496,  0.02185038,  0.77272234,  0.1306771 ,  0.12037412,\n",
       "        0.29978255,  0.52901495,  0.37234397,  0.68643679,  0.1017484 ,\n",
       "        0.92622153,  0.08885148,  0.88775519,  0.8884795 ,  0.92871526,\n",
       "        0.51789079,  0.25844172,  0.73236822,  0.36933681,  0.87765359,\n",
       "        0.267977  ,  0.79540464,  0.37479182,  0.93382436,  0.22085807,\n",
       "        0.77020062,  0.47554461,  0.2057005 ,  0.88257251,  0.88359478,\n",
       "        0.96602367,  0.38098845,  0.71077271,  0.22553066,  0.6232088 ,\n",
       "        0.84808517,  0.33087629,  0.75419759,  0.37899167,  0.21376518,\n",
       "        0.36550256,  0.35663026,  0.22663324,  0.88689502,  0.9532321 ,\n",
       "        0.74379125,  0.48210277,  0.80024987,  0.02282565,  0.93709519,\n",
       "        0.57010721,  0.79742379,  0.22398106,  0.41631334,  0.23409525,\n",
       "        0.95241079,  0.36539649,  0.02884495,  0.70723105,  0.90814692,\n",
       "        0.64168628,  0.70424713,  0.39240979,  0.03605938,  0.21163643,\n",
       "        0.56521922,  0.70240337,  0.05264655,  0.62080835,  0.92408775,\n",
       "        0.5189268 ,  0.99198453,  0.36417899,  0.93989896,  0.81655756,\n",
       "        0.9775063 ,  0.9072545 ,  0.88427272,  0.45472743,  0.23590427,\n",
       "        0.19066361,  0.54903849,  0.14104344,  0.94798356,  0.90049972,\n",
       "        0.49536177,  0.70738029,  0.51285591,  0.70565969,  0.25044466,\n",
       "        0.22118308,  0.78842596,  0.25540706,  0.89580781,  0.88312414,\n",
       "        0.81459042,  0.14947033,  0.79655981,  0.45577293,  0.30575156,\n",
       "        0.90970905,  0.00578121,  0.73519254,  0.08389065,  0.22098263,\n",
       "        0.49849212,  0.90944395,  0.91460997,  0.61483526,  0.60166575,\n",
       "        0.48559385,  0.60887444,  0.53825244,  0.166466  ,  0.54258106,\n",
       "        0.86032682,  0.46718744,  0.3367584 ,  0.98615532,  0.06676948,\n",
       "        0.56202342,  0.36067561,  0.50148639,  0.91621173,  0.57946066,\n",
       "        0.33652613,  0.13342111,  0.36282448,  0.58245056,  0.1621357 ,\n",
       "        0.95388307,  0.8275136 ,  0.64200422,  0.29675954,  0.85136772,\n",
       "        0.22863946,  0.98108481,  0.17078542,  0.10982992,  0.75506474,\n",
       "        0.64890945,  0.37479817,  0.98978579,  0.73688519,  0.94111242,\n",
       "        0.62989359,  0.32981319,  0.05042092,  0.5243859 ,  0.38125073,\n",
       "        0.69279574,  0.9099508 ,  0.5817359 ,  0.59493173,  0.65607429,\n",
       "        0.17132318,  0.01271646,  0.70196277,  0.02201228,  0.72624556,\n",
       "        0.47388854,  0.28099671,  0.79965934,  0.26257121,  0.99480903,\n",
       "        0.66584041,  0.99060838,  0.4190722 ,  0.4439386 ,  0.64318022,\n",
       "        0.11906864,  0.44635475,  0.32221026,  0.39337483,  0.06369434,\n",
       "        0.48387889,  0.88643726,  0.84426217,  0.56181175,  0.9255986 ,\n",
       "        0.78308543,  0.08747264,  0.88312069,  0.29030503,  0.67986269,\n",
       "        0.16204963,  0.48347925,  0.63001267,  0.39602069,  0.33872758,\n",
       "        0.37213788,  0.22618759,  0.9040593 ,  0.92547455,  0.26853697,\n",
       "        0.39066797,  0.70764266,  0.55621097,  0.08891266,  0.745104  ,\n",
       "        0.79957762,  0.08429801,  0.51634158,  0.59573753,  0.79075263,\n",
       "        0.85817761,  0.76052029,  0.03462697,  0.98762582,  0.53472842,\n",
       "        0.90915435,  0.29485382,  0.49336056,  0.09629008,  0.26150769,\n",
       "        0.42154716,  0.09314863,  0.7368707 ,  0.52672599,  0.20636783,\n",
       "        0.65500773,  0.86360247,  0.34361668,  0.05106933,  0.57634142,\n",
       "        0.60946514,  0.9139112 ,  0.28153674,  0.76708556,  0.2175453 ,\n",
       "        0.78479619,  0.1280144 ,  0.38260863,  0.24898395,  0.3059266 ,\n",
       "        0.00877599,  0.94543952,  0.86808753,  0.93240137,  0.08328511,\n",
       "        0.72820965,  0.69689039,  0.31993243,  0.39386381,  0.53727282,\n",
       "        0.50672122,  0.49371803,  0.94443734,  0.93036221,  0.31228568,\n",
       "        0.12268643,  0.98935764,  0.53985095,  0.84304908,  0.18350322,\n",
       "        0.41711309,  0.72053449,  0.03938548,  0.39397676,  0.7260949 ,\n",
       "        0.52238115,  0.08606217,  0.79945974,  0.94527234,  0.07913417,\n",
       "        0.13627567,  0.4186072 ,  0.09507945,  0.38840991,  0.40916104,\n",
       "        0.29440008,  0.57691869,  0.05477548,  0.05786491,  0.87596439,\n",
       "        0.72109282,  0.41694346,  0.4794534 ,  0.93954701,  0.11298926,\n",
       "        0.0099573 ,  0.68506181,  0.56431101,  0.79154306,  0.43002554,\n",
       "        0.0082428 ,  0.12183718,  0.8583567 ,  0.49985339,  0.86711301,\n",
       "        0.82442435,  0.00699195,  0.41212428,  0.725609  ,  0.17623939,\n",
       "        0.06932771,  0.82509049,  0.01180371,  0.61985543,  0.63152501,\n",
       "        0.50919986,  0.27823771,  0.31347072,  0.0754803 ,  0.61268123,\n",
       "        0.78623189,  0.84150953,  0.62360872,  0.94981275,  0.52491487,\n",
       "        0.82527901,  0.93333844,  0.03606147,  0.69099582,  0.12643067,\n",
       "        0.56268761,  0.88295059,  0.67152722,  0.34291374,  0.75726061,\n",
       "        0.58149353,  0.96443727,  0.89464988,  0.65029778,  0.48792212,\n",
       "        0.77870282,  0.42501401,  0.60427121,  0.55337305,  0.47438373,\n",
       "        0.20648307,  0.46884188,  0.47701131,  0.89531901,  0.50044191,\n",
       "        0.55155147,  0.75859857,  0.35819444,  0.13041852,  0.11405194,\n",
       "        0.32814743,  0.3640141 ,  0.17568411,  0.2785193 ,  0.07627548,\n",
       "        0.07257415,  0.11438974,  0.22121292,  0.80105437,  0.94613581,\n",
       "        0.21466139,  0.98782407,  0.27372352,  0.3124899 ,  0.22338404,\n",
       "        0.95569519,  0.85844277,  0.6894855 ,  0.46862004,  0.99746058,\n",
       "        0.30436565,  0.79159917,  0.11307586,  0.74805465,  0.90160063,\n",
       "        0.13697911,  0.93607482,  0.96993936,  0.89294083,  0.44630754,\n",
       "        0.12859009,  0.27177997,  0.40828902,  0.61383794,  0.13915043,\n",
       "        0.22836496,  0.03243922,  0.15519703,  0.50331781,  0.28450191,\n",
       "        0.20967654,  0.3223502 ,  0.62668155,  0.29604396,  0.72521965,\n",
       "        0.28800477,  0.38329121,  0.70080112,  0.88783553,  0.87292248,\n",
       "        0.92846129,  0.29342307,  0.82635848,  0.36148094,  0.59451617,\n",
       "        0.3140278 ,  0.84112406,  0.02882541,  0.74875365,  0.71510751,\n",
       "        0.66013575,  0.58603849,  0.71873581,  0.86851458,  0.63758634,\n",
       "        0.79583427,  0.26707866,  0.17418359,  0.04690631,  0.55444693,\n",
       "        0.18546366,  0.99757599,  0.15379068,  0.53772958,  0.83068872,\n",
       "        0.76522127,  0.50585932,  0.34434812,  0.14969826,  0.52286876,\n",
       "        0.52341456,  0.94464607,  0.25480312,  0.36766719,  0.64564833,\n",
       "        0.19289811,  0.79073942,  0.74544044,  0.02698079,  0.6088013 ,\n",
       "        0.31664037,  0.20141858,  0.76932635,  0.91665736,  0.77696496,\n",
       "        0.11395809,  0.35227977,  0.63771413,  0.63489446,  0.91337477,\n",
       "        0.75028688,  0.64742589,  0.88212865,  0.41348304,  0.87847451,\n",
       "        0.77321882,  0.11634708,  0.98041423,  0.0507959 ,  0.03034643,\n",
       "        0.88444043,  0.62771162,  0.94488342,  0.09143671,  0.62272892,\n",
       "        0.64109032,  0.9311582 ,  0.44781917,  0.77390541,  0.14529893,\n",
       "        0.95661866,  0.44137019,  0.9355511 ,  0.96435093,  0.21924849,\n",
       "        0.80555017,  0.84526392,  0.96288498,  0.792236  ,  0.73190959,\n",
       "        0.3615604 ,  0.54729119,  0.44906092,  0.20201042,  0.1094682 ,\n",
       "        0.0663962 ,  0.48980209,  0.68773778,  0.88511763,  0.55153096,\n",
       "        0.02199181,  0.33880676,  0.25915808,  0.96432429,  0.80318385,\n",
       "        0.71553403,  0.01873938,  0.31643849,  0.68143896,  0.6484045 ,\n",
       "        0.14388565,  0.86403966,  0.4521611 ,  0.4186666 ,  0.61616757,\n",
       "        0.19644757,  0.55078644,  0.86609117,  0.0101422 ,  0.07307138,\n",
       "        0.61021467,  0.06070061,  0.12070677,  0.09624016,  0.69255796,\n",
       "        0.90289675,  0.91694194,  0.44328569,  0.20711763,  0.60796626,\n",
       "        0.91347481,  0.12395044,  0.55680034,  0.83603005,  0.16952755,\n",
       "        0.37096932,  0.63391296,  0.44908309,  0.67733198,  0.98793522,\n",
       "        0.38373127,  0.25843836,  0.63496755,  0.86146647,  0.11025708,\n",
       "        0.48519487,  0.3744652 ,  0.43666692,  0.19992817,  0.59739824,\n",
       "        0.75419019,  0.71046752,  0.02554375,  0.44245118,  0.03227589,\n",
       "        0.85174281,  0.4951263 ,  0.5043667 ,  0.96293197,  0.22192923])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "N = 1000\n",
    "dtype = np.float32\n",
    "np.random.sample(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
